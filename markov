#!/usr/bin/env nix-shell
#!nix-shell -i python3 -p "python3.withPackages (ps: [ps.docopt])"

"""Markov Text Generator.

Usage:
  markov [--stats] [-l <len>] [-n <n>] [-t <file>] [--] (<weight> <file>)...

Options:
  --stats   print statistics about the parsed corpora
  -l <len>  length of string to generate [default: 350]
  -n <n>    ngram length [default: 3]
  -t <file> thesaurus file (lines in the format "alternate, alternate, ... => normal form")
"""

import dataclasses
import pathlib
import random
import re
import string
import typing

COLOURS: list[str] = [
    "\033[31m",  # red
    "\033[32m",  # green
    "\033[33m",  # yellow
    "\033[34m",  # blue
    "\033[35m",  # purple
    "\033[36m",  # cyan
]

COLOUR_RESET: str = "\033[0m"

STR_TABLE: dict[int, int | None] = str.maketrans(
    string.ascii_uppercase, string.ascii_lowercase, string.punctuation
)


@dataclasses.dataclass
class Thesaurus:
    substitutions: dict[str, str] = dataclasses.field(default_factory=dict)
    regexen: list[tuple[re.Pattern[str], str]] = dataclasses.field(default_factory=list)

    @classmethod
    def from_str(cls, source: str) -> typing.Self:
        substitutions = {}
        regexen = []
        for line in source.splitlines():
            lhs, rhs = line.split("=>")
            alts = [a.strip() for a in lhs.split(",")]
            normal_form = rhs.strip()
            for alt in alts:
                if alt[0] == "/" and alt[-1] == "/":
                    pattern = re.compile(alt[1:-1])
                    regexen.append((pattern, normal_form))
                else:
                    substitutions[alt] = normal_form
        return cls(substitutions=substitutions, regexen=regexen)

    @classmethod
    def from_file(cls, path: pathlib.Path) -> typing.Self:
        return cls.from_str(path.read_text())

    def normalise(self, token: str) -> str:
        for pattern, normal_form in self.regexen:
            if re.fullmatch(pattern, token):
                return normal_form

        transformed = token.translate(STR_TABLE)
        return self.substitutions.get(transformed, transformed)


@dataclasses.dataclass
class Markov:
    # TODO: would be nice to show that the tuple types in `data`, `train`, and
    # `generate` are all the same type
    ngram_len: int = 3
    data: dict[tuple[str | None, ...], dict[tuple[str, str], int]] = dataclasses.field(
        default_factory=dict
    )
    thesaurus: Thesaurus = dataclasses.field(default_factory=Thesaurus)

    def train(self, tokens: list[str], corpus: str, weight: int = 1):
        ngram = tuple([None for _ in range(self.ngram_len)])
        for token in tokens:
            # store the original token for text generation
            followers = self.data.setdefault(ngram, {})
            followers.setdefault((token, corpus), 0)
            followers[(token, corpus)] += weight

            # but then use the normalised token in the ngram
            ngram += (self.thesaurus.normalise(token),)
            ngram = ngram[1:]

    def generate(self, length: int, switch_chance: int = 2) -> list[str]:
        ngram = random.choice(list(self.data.keys()))
        out = []
        last_corpus = None
        tokens_in_same_corpus = 0
        for _ in range(length):
            candidates = self.data[ngram]

            # force choosing a token from a different corpus if we've picked too
            # many from the same one in a row
            if random.randrange(0, 100) <= switch_chance * tokens_in_same_corpus:
                filtered_candidates = {
                    (token, corpus): weight
                    for (token, corpus), weight in candidates.items()
                    if corpus != last_corpus
                }
                if filtered_candidates:
                    candidates = filtered_candidates

            token, corpus = weighted_choice(candidates)

            # improve the "too many from the same one in a row" check by pretending
            # this token came from the prior corpus, if it could have
            if corpus != last_corpus and (token, last_corpus) in self.data[ngram]:
                corpus = last_corpus

            out.append((token, corpus))

            # use the normalised token in the ngram
            ngram += (thesaurus.normalise(token),)
            ngram = ngram[1:]

            if corpus == last_corpus:
                tokens_in_same_corpus += 1
            else:
                last_corpus = corpus
                tokens_in_same_corpus = 1

        return out


def tokenise(text: str) -> list[str]:
    return text.split()


# black doesn't support the newer syntax:
#
#     def weighted_choice[T](freqdict: dict[T, int]) -> T:
T = typing.TypeVar("T")


def weighted_choice(freqdict: dict[T, int]) -> T:
    items = freqdict.items()
    return random.choices([k for k, _ in items], weights=[w for _, w in items])[0]


if __name__ == "__main__":
    import docopt
    import itertools

    args = docopt.docopt(__doc__)
    output_len = int(args["-l"])
    ngram_len = int(args["-n"])
    thesaurus_file = args["-t"]
    corpora = list(zip([int(w) for w in args["<weight>"]], args["<file>"]))

    thesaurus = (
        Thesaurus() if thesaurus_file is None else Thesaurus.from_file(pathlib.Path(thesaurus_file))
    )

    corpus_colour = {}
    markov = Markov(ngram_len=ngram_len, thesaurus=thesaurus)
    all_tokens = set()
    for colour, (cweight, cfile) in zip(itertools.cycle(COLOURS), corpora):
        tokens = tokenise(pathlib.Path(cfile).read_text())
        all_tokens |= set(tokens)
        corpus_colour[cfile] = colour
        markov.train(tokens, cfile, weight=cweight)

    if output_len > 0:
        generated = [
            f"{corpus_colour[corpus]}{token}{COLOUR_RESET}"
            for (token, corpus) in markov.generate(output_len)
        ]
        print(" ".join(generated))

    if args["--stats"]:
        print()

        normalised_tokens = set(thesaurus.normalise(tok) for tok in all_tokens)
        print(f"{len(all_tokens)} tokens: {len(normalised_tokens)} normalised")

        ambiguous = 0
        unambiguous = 0
        ngrams = markov.data
        for ngram, ngram_data in ngrams.items():
            if len(ngram_data) == 1:
                unambiguous += 1
            else:
                ambiguous += 1
        print(f"{len(ngrams)} ngrams: {ambiguous} ambiguous, {unambiguous} unambiguous")
